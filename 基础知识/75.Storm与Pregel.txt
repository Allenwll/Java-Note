			Storm与Pregel

1.批处理与流计算
	
	对静态数据和流数据的处理,对应着两种截然不同的计算模式:批处理和流(实时)计算
	
2.流计算概念:
			
			
	实时获取来自不同数据源的海量数据,经过实时分析处理,获取有价值的信息
		
	数据的价值随着时间的流逝而降低
		
3.流计算需求
		
	通用：支持多种流数据类型的处理
			
	高性能:能够处理体量大、流速快的数据
			
	高可靠:能可靠地处理流数据
			
	可扩展:可通过增加节点扩展系统性能
			
	廉价:可以运行在通用硬件之上
			
	易用性:能够快速进行开发和部署
			
4.流计算处理流程
	
	传统的数据处理流程
		
		先采集数据并存储在关系数据库等数据管理系统之中,之后由用户通过查询操作和数据管理系统进行交互
		
		存储的数据是旧的.
		
		需要用户主动发出查询来获取结果
		
	流计算的处理流程
		
		数据实时采集
		
			通常采集多个数据源的海量数据,需要保证实时性、低延迟与稳定可靠
			
				分布式日志采集系统
					
					Facebook的Scribe
					
					LinkedIn的Kafka
					
					淘宝的Time Tunnel
					
					基于Hadoop的Chukwa和Flume
					
			数据采集系统的基本架构
				
				Agent:
					
					主动采集数据,并把数据推送到Collector部分
					
				Collector:
					
					接收多个Agent的数据,并实现有序、可靠、高性能的转发
					
				Store:
					
					存储Collector转发来的数据
			
		数据实时计算
			
			对采集的数据进行实时的分析和计算,并反馈实时结果
			
			经流处理系统处理后的数据,可视情况进行存储,以便之后再进行分析计算
			
			在时效性要求较高的场景中,处理之后的数据也可以直接丢弃
			
		实时查询服务
		
			经由流计算框架得出的结果可供用户进行实时查询、展示或存储
			
	不同之处：
	
		流处理系统处理的是实时的数据
			
		传统的数据处理系统处理的是预先存储好的静态数据

		用户通过流处理系统获取的是实时结果
		
		传统的数据处理系统获取的是过去某一时刻的结果
		
		流处理系统无需用户主动发出查询,实时查询服务可以主动将实时结果推送给用户
		
5.开源流计算系统
	
	Storm
	
		Apache Storm最初由Nathan Marz以及他的BackType的团队在2010年创建,后来被Twitter收购并开源
		
		在2014年成了Apache的顶层项目,是一个典型的Native Streming系统并且提供了大量底层的操作接口
	
	Trident Storm
		
		是一个基于Storm构建的上层的Micro-Batching系统
	
		简化了Storm的拓扑构建过程并且提供了类似于窗口、聚合以及状态管理等未被Storm原生支持的功能
		
	Spark Streming
	
		提供了类似于SparkSQL、Mlib这样内建的批处理框架的库,并且它也提供了Spark Streaming流处理框架
	
		Spark的运行环境提供了批处理功能,因此Spark Streaming毫无疑问是实现了Micro-Batching机制
	
		输入的数据流会被接收者分割创建为Micro-Batches,然后像其他Spark任务一样进行处理
	
	Samza
		
		最早是由LinkedIn提出的与Kafka协同工作的优秀地流解决方案,Samza已经是LinkedIn内部关键的基础设施之一
	
		Samza重负依赖于Kafaka的基于日志的机制,二者结合地非常好
		
	Flink
		
		Flink也是一个Native Streaming的系统,并且提供了大量高级别的API
	
		FLink也像Spark一样提供了批处理的功能,可以作为流处理的一个特殊案例来看
	
		
	
6.Storm
	
	概念
	
		是一个免费、开源的分布式实时计算系统
	
	特点
	
		简单编程模型
		
			类似于MapReduce降低了并行批处理复杂性,Storm降低了进行实时处理的复杂性
			
		可使用各种编程语言
			
			可以在Storm之上使用各种编程语言,支持Java、Ruby和Python等
			
			要增加对其他语言的支持,只需实现简单的Storm通信协议即可
			
		快速
			
			系统底层基于消息队列技术,保证"消息"能得到快速地处理
			
		可靠
			
			Storm保证每个"消息"至少能得到一次完整处理
			
		可扩展
			
			计算是在多个线程、进程和服务器之间并行进行的,可通过增加节点来提高系统处理能力
			
		容错
		
			Storm会管理工作进程和节点的故障
			
7.Storm术语
	
	Streams
		
		Storm将流数据Stream描述成一个无限的Tuple序列,这些Tuple序列会以分布式的方式并行地创建和处理
			
			每个Tuple是一堆值,每个值有一个名字,并且每个值可以是任何类型,字段名称已经事先定义好了
			
			所以Tuple只需要按序填入各个Value,所以就是一个Value List(值列表)
	Spout
		
		Storm认为每个Stream都有一个源头,并把这个源头抽象为Spout
		
			通常Spout会从外部数据源(队列、数据库等)读取数据,然后封装成Tuple形式,发送到Stream中
			
			Spout是一个主动角色,在接口内部有个nextTuple函数,Storm框架会不停的调用该函数
			
	Bolt
		
		Storm将Streams的状态转换过程抽象为Bolt,Bolt即可以处理Tuple,也可以将处理后的Tuple作为新的Streams发送给其他Bolt
		
			Bolt可以执行过滤、函数操作、Join等任何操作
			
			Bolt是一个被动角色,其接口中有一个execute(Tuple input)方法,在接收到消息之后会调用此函数,用户可以在此方法中执行自己的处理逻辑
			
	Topology
		
		Storm将Spouts和Bolts组成的网络抽象成Topology,它可以被提交到Storm集群执行
		
		Toplogy可视为流转换图,图中节点是一个Spout或Bolt,边则表示Bolt订阅了哪个Stream
		
		当Spout或者Bolt发送元组时,它会把元组发送到每个订阅了该Stream的Bolt上进行处理
			
			Topology里面的每个处理组件(Spout或Bolt)都包含处理逻辑,而组件之间的连接表示数据流动的方向
			
			在Topology里面可以指定每个组件的并行度,Storm会在集群里面分配那么多的线程来同时计算
			
	Stream Grouping
		
		Storm中的Stream Groupings用于告知Topology如何在两个组件间(如Spout和Bolt之间,或者不同的Bolt之间)进行Tuple的传送
		
			每一个Spout和Bolt都可以有多个Task、一个Task在什么时候、以什么方式发送Tuple就是由Stream Grouping来决定的
				
		方式:
			
			1.ShuffleGrouping
				
				随机分组,随机分发Stream中的Tuple,保证每个Bolt接收Tuple数量大致一致
				
			2.FieldsGrouping
				
				按照字段分组,保证相同字段的Tuple分配到同一个Bolt中
			
			3.AllGrouping
				
				广播发送,每一个Bolt都会收到所有的Tuple
			
			4.GlobalGroup
					
				全局分组,所有的Tuple都发送到同一个Bolt的某一个Task中
			
			5.NonGrouping
				
				不分组,和ShuffleGrouping类似
				
			6.DirectGrouping
				
				直接分组,直接指定由哪个Bolt的哪个Task来执行Tuple的处理

8.Storm架构
	
	Storm集群采用"Master-Slave"架构
		
		Master节点运行名为"Nimbus"的后台程序(类似Hadoop中的"JobTracker"),负责在集群范围内分发代码、为Slave分配任务和监测故障
		
		Slave节点运行名为"Supervisor"的后台程序,负责根据Nimbus分配的任务来决定启动或停止Worker进程,一个Slave节点上同时运行若干个Worker进程
		
	Storm使用Zookeeper来作为分布式协调组件,负责Nimbus和多个Supervisor之间的所有协调工作
	
	借助于Zookeeper,若Nimbus进程或Supervisor进程意外终止,重启时也能读取、恢复之前的状态并继续工作
	
9.Storm工作流程
		
	1.Storm客户端节点提交向Storm(Master Node)提交Topology
	
	2.Nimbus首先将提交的Topology进行分片,分成一个Task
	
		分配给相应的Supervisor,并将Task和Supervisor相关的信息提交到Zookeeper集群上
	
	3.Supervisor在Zookeeper集群上认领自己的Task,通知自己的Worker进程进行Task的处理
	
	
10.图计算概述
		
	图是计算机科学中的经典问题,在现实中也有很多应用与图相关,如电子地图中的路径、Web连接组成的图
	
	典型的图计算算法有最短路径计算、二分匹配、PageRank等,具有广泛应用
			
	目前针对图计算的解决方案中,单击图算法库比较成熟,但扩展性不好,很难处理Web规模的图数据
		
	并行图算法库又缺少容错,然而在通用硬件上进行大规模图数据处理出错是常态
		
	基于MapReduce的多轮迭代不够高效,因为需要反复存储和读取迭代结果
		
	用MPI实现并行计算比较灵活,但需要应用考虑通信、同步、容错等,编程复杂
		
	缺乏一个支持多种图计算算法实现,并且高效率、可扩展、错误容忍的分布式图处理系统
		
11.大规模图计算系统
		
	Google Pregel
		
	Apache Griaph
		
	Apache Hama
		
	Spark GraphX
		
	CMU GraphLab

12.Pregel
	
	Pregel是Google提出的大规模分布式图计算系统,专门用来解决网页链接分析、社交数据挖掘等实际应用中涉及的大规模分布式图计算问题
	
	特点:
		
		基于BSP(Bulk Synchoronous Parallel,整体同步并行)模型
		
		以图节点为计算中心
		
		高效率、可扩展、容错、支持多种图计算算法的实现
		
13.BSP模型
	
	整体同步并行计算模型Bulk Synchoronous Parallel Computing Model 简称BSP,又名大同步模型,由哈佛大学Viliant和牛津大学Bill McColl在上世纪80年代提出
	
	在BSP模型中,计算过程包括一系列全局超步(所谓的超步就是计算中得一次迭代),每个超步主要包括三个组件:
		
		1.局部计算
			
			每个参与的处理器都有自身的计算任务,它们只读取存储在本地内存中的值,不同处理器的计算任务都是异步并且独立的
			
		2.通讯
			
			处理器群相互交换数据,交换的形式是由一方发起推送(put)或获取(get)操作
			
		3.栅栏同步(Barries Synchronization)
			
			当一个处理器遇到"路障"或栅栏,会等到其他所有处理器完成它们的计算步骤
			
			每一次同步也是一个超步的完成和下一个超步的开始
			
14.Pregel计算模型
	
	以有向图作为输入
		
	有向图的每个顶点都有一个String类型的顶点ID
	
	每个顶点都有一个可修改的用户自定义值与之关联
	
	每条有向边都和其源顶点关联,并记录了其目标顶点ID
	
	边上有一个可修改的用户自定义值与之关联
	
	在Pregel计算过程中,算法在何时结束是由所有顶点的状态决定的,在第0个超步,所有的顶点处于活跃状态,都会参与该超步的计算过程
		
	当一个顶点不需要继续执行进一步的计算时,就会把自己的状态设置为停机,进入非活跃状态
		
	一旦一个顶点进入非活跃状态,后续超步中就不会再在该顶点上执行计算,除非其他顶点给该顶点发送消息把它再次激活

	当所有的顶点都已经标识其自身达到inactive非活跃状态,并且没有消息在传送的时候,算法就可以停止运行

15.一个Pregel用户程序的执行过程如下:
		
	1.选择集群中的多台机器执行图计算任务,每台机器上运行用户程序的一个副本,其中有一台机器会被

		
		
		
		
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
			
	
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
	
	
	
 