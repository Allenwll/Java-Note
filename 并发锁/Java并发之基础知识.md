### 语法糖

> 语法糖（Syntactic Sugar），也称糖衣语法，是由英国计算机学家 Peter.J.Landin 发明的一个术语，指在计算机语言中添加的某种语法。
> 
> 这种语法对语言的功能并没有影响，但是更方便程序员使用。简而言之，语法糖让程序更加简洁，有更高的可读性。
> 
> 例如泛型、switch支持string、自动拆装箱、可变参数、枚举、内部类等

### 信号量

> 信号量(Semaphore)，有时被称为信号灯，是在多线程环境下使用的一种设施，是可以用来保证两个或多个关键代码段不被并发调用

### 互斥量

> 互斥量是一个可以处于两态之一的变量：解锁和加锁。
> 
> 如果不需要信号量的计数能力，有时可以使用信号量的一个简化版本，称为互斥量（mutex）。互斥量仅仅适用于管理共享资源或一小段代码。

#### 互斥量和信号量的区别

*   互斥量用于线程的互斥，信号量用于线程的同步。

    > 这是互斥量和信号量的根本区别，也就是互斥和同步之间的区别。
> 
>     互斥：是指某一资源同时只允许一个访问者对其进行访问，具有唯一性和排它性。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。
> 
>     同步：是指在互斥的基础上（大多数情况），通过其它机制实现访问者对资源的有序访问。在大多数情况下，同步已经实现了互斥，特别是所有写入资源的情况必定是互斥的。少数情况是指可以允许多个访问者同时访问资源

*   互斥量值只能为0/1，信号量值可以为非负整数。

    > 也就是说，一个互斥量只能用于一个资源的互斥访问，它不能实现多个资源的多线程互斥问题。信号量可以实现多个同类资源的多线程互斥和同步。当信号量为单值信号量是，也可以完成一个资源的互斥访问。

*   互斥量的加锁和解锁必须由同一线程分别对应使用，信号量可以由一个线程释放，另一个线程得到。

## 并发的命令式编程中线程之间的通信机制

> 1.共享内存
> 
> > 线程之间共享程序的公共状态，线程之间通过读-写内存中的公共状态来隐式进行通信
> 
> 2.消息传递
> 
> > 线程之间没有公共状态，必须通过明确的发送消息来显式通信

同步是指程序用于通知不同线程之间操作发生相对顺序的机制

> 在共享内存并发模型里，同步是显式进行的，程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行
> 
> 在消息传递的并发模型里，由于消息的发送必须在接收之前，因此同步是隐式进行的

Java并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明

## 管程

1.  管程可以看做一个软件模块，它是将共享的变量和对于这些共享变量的操作封装起来，形成一个具有一定接口的功能模块，进程可以调用管程来实现进程级别的并发控制。

2.  进程只能互斥得使用管程，即当一个进程使用管程时，另一个进程必须等待。当一个进程使用完管程后，它必须释放管程并唤醒等待管程的某一个进程。

3.  在管程入口处的等待队列称为入口等待队列，由于进程会执行唤醒操作，因此可能有多个等待使用管程的队列，这样的队列称为紧急队列，它的优先级高于等待队列

### 特色

> 在编程语言中对并发的细节进行封装，使程序员可以直接在语言中就得到并发的支持，而不必自己去处理一些像是控制信号量之类容易出错且频繁的细节问题
> 
> 一些语言是通过在编译时解开语法糖的方式去实现管程，但Java在编译后生成的字节码层面上对并发仍然是一层封装
> 
> > 比如 syncrhonized块在编译之后只是对应了两条指令：monitorenter和monitorexit。更多的并发细节是在JVM运行时去处理 的，而不是编译

### CPU优化：

> 目标:CPU执行单元的速度要远超主存访问速度
> 
> CPU避免内存访问延迟最常见的技术是将指令管道化，然后尽量重排这些管道的执行以最大化利用缓存，从而把因为缓存未命中引起的延迟降到最小

## JAVA内存模型  JMM

> Java内存模型所规范的是数据在线程自己的独立内存空间和JVM共享内存之间同步的问题
> 
> JMM决定一个线程对共享变量的写入何时对另一个线程可见
> 
> 多核CPU和高速缓存在JVM中对应的是Java语言内置的线程和每个线程所拥有的独立内存空间

### 主内存

> Java内存模型规定，对于多个线程共享的变量，存储在主内存当中，每个线程都有自己独立的工作内存，线程只能访问自己的工作内存，不可以访问其它线程的工作内存

### 工作内存

> 工作内存中保存了主内存共享变量的副本，线程要操作这些共享变量，只能通过操作工作内存中的副本来实现，操作完毕之后再同步回到主内存当中

### 工作内存和主内存交互协议

#### 先定义了8种原子操作

1.lock:将主内存中的变量锁定，为一个线程所独占

2.unclock:将lock加的锁定解除，此时其他线程可以访问此变量

3.read:将主内存中的变量值读到工作内存当中

4.load:将read读取的值保存到工作内存中的变量副本中

5.use:将值传的给现成的代码执行引擎

6.assign:将执行引擎处理返回的值重新赋值给变量副本

7.store:将变量副本的值存储到主内存中

8.write:将store存储的值写入到主内存的共享变量中

即要保证数据的同步，需要成对指令出现，如果只有单一指令会造成数据不一致

#### 规则

1read和load、store和write必须要成对出现，不允许单一操作

2.在线程中使用了assign操作改变了变量副本,那么必须把这个副本通过store-write同步回主内存中，如果没有发生assign操作，那么不允许使用store-write

3.在对一个变量实行use和store操作之前，必须实行过load和assign操作

4.变量在同一时刻只允许一个线程对其进行lock，有多少次load就必须有多少次unlock

5.在lock操作之后会清空副本，需要再次从主内存总read-load新的值。在执行unlock前，需要把改变的副本同步到主存

## 内存可见性

volatile保证可见性的原理是在每次访问变量时都会进行一次刷新，因此每次访问都是主存中最新的版本

## 指令重排序

> 目的是为了优化性能，编译器和处理器会对指令做重排序
> 
> 假设有2个共享变量a和b,在线程A中对2个变量进行赋值操作，假设当A对a进行赋值操作的时候发现a在主内存中已经被其他线程加锁，那么它会去尝试进行b的赋值操作，如果没被占用，会先为b赋值，执行顺序就会改变
> 
> 单线程没问题，多线程会有问题
> 
> 需要先保证程序的正确然后再优化性能。
> 
> > 解决重排序问题通过volatile关键字，volatile能确保变量在线程中的操作不会被重排序而是按照代码中规定的顺序进行访问

### 重排序分三种类型

1.编译器优化的重排序

> 编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序

2.指令级并行的重排序

> 现代处理器采用了指令级并行技术(Instruction-Level Parallelism )ILP来将多条指令重叠进行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序

3.内存系统的重排序

> 由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去是在乱序进行

从Java源代码到最终实际执行的指令序列会分别经历3种重排序:
源代码-&gt;编译器优化重排序-&gt;指令级并行重排序-&gt;内存系统重排序-&gt;最终执行的指令序列

> 这些重排序都可能会导致多线程出现内存可见性问题
> 
> 对于编译器，JMM的编译器重排序规则会禁止**特定类型**的编译器重排序
> 
> 对于处理器重排序,JMM的处理器重排序规则会要求java编译器在生成指令序列时，插入特定类型的**内存屏障**，通过内存屏障指令来禁止特定类型的处理器重排序

JMM属于语言的内存模型，确保在不同的编译器和不同的处理器平台上，通过禁止特定类型的编译器重排序和处理器重排序，提供一致的内存可见性保证

## 内存屏障

> 让一个CPU处理单元中的内存状态对其它处理单元可见的一项技术
> 
> > **代码顺序并不是真正的执行顺序，只要有空间提高性能，CPU和编译器可以进行各种优化。缓存和主存的读取会利用load, store和write-combining缓冲区来缓冲和重排**
> 
> 一旦内存数据被推送到缓存，就会有消息协议来确保所有的缓存会对所有的共享数据同步并保持一致。这个使内存数据对CPU核可见的技术被称为内存屏障或内存栅栏

### 内存屏障2个功能

> *   通过确保从另一个CPU来看屏障两边的所有指令都是正确的程序顺序，从而确保程序顺序的外部可见性
> 
> *   可以实现内存数据的可见性,确保内存数据会同步到CPU缓存子系统

## X86架构下的内存屏障

### Store Barrier

> Store屏障是x86的“**sfence**”指令。强制所有在store屏障指令之前的sotre指令，都在该其执行之前被执行，并把store缓冲区的数据都刷新到CPU缓存
> 
> 会使得程序状态对其他CPU可见，这样其他CPU可以根据需要介入

### Load Barrier

> Load屏障，是x86的"**ifence**"指令。强制所有在load屏障指令之后load指令都该在其执行后被执行，并且一直等到load缓冲区被该CPU读完才能执行之后的load指令
> 
> 使得从其他CPU暴露出来的程序状态对该CPU可见，这之后CPU可以进行后续处理

### Full Barrier

> Full屏障，是x86的"**mfence**"指令，复合了load和store屏障的功能
> 
> > **原子指令，如x86上的"lock..."指令是一个Full Barrier，执行时会锁住内存子系统来确保执行顺序，甚至跨多个CPU**
> > 
> > > Software Locks通常使用了内存屏障或原子指令来实现变量可见性和保持程序顺序

**Java内存模型中volatile变量在写操作之后会插入一个store屏障，在读操作之前会插入会插入一个load屏障**

**一个类的final字段会在初始化后插入一个store屏障，来确保final字段在构造函数初始化完成并可被使用时可见**

### 内存屏障的性能影响

> 内存屏障阻碍了CPU采取优化技术来降低内存操作延迟，必须考虑因此带来的性能损失
> 
> 解决方案:把要解决的问题模块化，这样处理器可以按单元执行任务，然后在任务单元的边界放上所有需要的内存屏障。

## happens-before

在JMM中，如果一个操作执行的结果要对另一个操作可见，那么这连个操作必须存在happens-before关系

两个操作既可以是在一个线程内，也可以是在不同线程之间

### 规则

*   程序顺序规则

    > 一个线程中的每个操作，happens-before于该线程中的任意后续操作

*   监视器锁规则

    > 对一个监视器锁的解锁，happens-before于随后对这个监视器锁的加锁

*   volatile变量规则

    > 对一个volatile域的写，happens-before于任意后续对这个volatile域的读

*   传递性

    > 如果 A happens-before B 且 happens-before C 那么A happens-before C

#### 注意，连个操作之间具有happens-before关系，并不意味者前一个操作必须要在后一个操作之前执行!

#### happens-before仅仅要求前一个操作(执行的结果)对后一个操作可见，且前一个操作顺序排在第二个操作之前

一个happens-before规则通常对应于多个编译器和处理器重排序规则，happens-before规则避免Java程序员为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现